---
title: "Predictive Analytics Binary Target Model Lab"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---

#### Course Number: IS-470
#### Course Title: Business Intelligence and Big Data
#### Student Name: ADD

# Introduction

Your orange juice company, OJ-R-US, just bought Citrus Hill and wants to continue selling orange juice to their newly acquired customer base. The purpose of this lab is two fold:

- Explore the `OJ` data.
- Identify predictors that are predictive in nature.
- Develop and compare the two binary target models.

To complete the lab successfully, run the code, edit the code to run your own analysis when prompted, and answer all the questions about the predictive analysis completed within the notebook.

# Load Packages and Examine Data

The following libraries will be used within the analysis. Remember to install the packages using the `Packages` tab on the right hand side before loading the packages.

```{r include=FALSE}
#install.packages(c('caret', 'skimr', 'RANN', 'randomForest', 'fastAdaboost', 'gbm', 'xgboost', 'caretEnsemble', 'C50', 'earth'))
library(caret)
library(tidyr)
library(dplyr)
library(ggplot2)
#library(GGally)
library(nycflights13)
library(RANN)
library(ISLR)
library(skimr)
library(Boruta)
library(fastDummies)
library(boot)
```

# Basic Exploritory Data Analysis

```{r, warning=FALSE}
summary(OJ)
```

```{r, warning=FALSE}
OJ %>% skimr::skim()
```

```{r, warning=FALSE}
#plot(OJ$Purchase,OJ$PriceCH)
```

# Data Preperation
```{r}
# Create the training and test datasets
set.seed(100)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(OJ$Purchase, p=0.8, list=FALSE)

# Step 2: Create the training  dataset
trainData <- OJ[trainRowNumbers,]

# Step 3: Create the test dataset
testData <- OJ[-trainRowNumbers,]

# Store X and Y for later use.
trainData_with_dummies = dummy_cols(
  trainData, select_columns = c('STORE', 'StoreID', 'Store7'),
  remove_selected_columns = TRUE)
# xtrain = subset(trainData, select = -c(Purchase) )
# y = trainData$Purchase
```

```{r}
print(dim(trainData_with_dummies))
```

# Varaible Selection

http://r-statistics.co/Variable-Selection-and-Importance-With-R.html

```{r, message=FALSE}
# prepare training scheme

control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(Purchase~., data=trainData_with_dummies, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
```

```{r}
# plot importance
plot(importance)
```

```{r}
# calculate correlation matrix
x_num = subset(trainData_with_dummies, select = -c(Purchase) )
correlationMatrix <- cor(as.matrix(x_num))
# find attributes that are highly corrected
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.85)
# print indexes of highly correlated attributes
print(names(x_num)[highlyCorrelated])
x_num_high_corr = subset(trainData_with_dummies, select = c(names(x_num)[highlyCorrelated]) )
correlationMatrix <- cor(as.matrix(x_num_high_corr))
print(correlationMatrix)
# dropping least important correlated numeric columns
trainDataReduce = subset(trainData_with_dummies, select = -c(DiscMM, PctDiscMM, StoreID_7, Store7_No) )
```
# Add stepwise

```{r}
# Decide if a variable is important or not using Boruta
boruta_output <- Boruta(Purchase ~ ., data=trainDataReduce, doTrace=2)  # perform Boruta search
boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% c("Confirmed")])
# boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% c("Confirmed", "Tentative")])  # collect Confirmed and Tentative variables
print(boruta_signif)  # significant variables
```

```{r}
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")  # plot variable importance
```

# Model Builds

```{r}
# Logistic regression using forward selection
#logistic_model_null = glm(Purchase ~ 1, data=trainDataReduce, family = binomial(link = "logit"))
#logistic_model_full = glm(Purchase ~ ., data=trainDataReduce, family = binomial(link = "logit"))
#OJ_steps = step(logistic_model_null,
#                scope = list(lower=logistic_model_null, 
#                             upper=logistic_model_full),
#                direction = "forward")

fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)

set.seed(13)
gbmFit1 <- train(Purchase ~ ., data = trainDataReduce, 
                 method = "gbm", 
                 trControl = fitControl,
                 ## This last option is actually one
                 ## for gbm() that passes through
                 verbose = FALSE)
gbmFit1
```

```{r}
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 2,
                     number = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     allowParallel = FALSE)
boostGrid <- expand.grid(nrounds=100,
                         max_depth=c(2,6,10),
                         eta=c(0.01,0.1),
                         gamma=c(0),
                         colsample_bytree=1,
                         min_child_weight=1,
                         subsample=0.7)
set.seed(13)
boostTuned <- train(Purchase ~ ., data=trainDataReduce,
                    method="xgbTree",
                    metric="ROC",
                    trControl=ctrl,
                    tuneGrid=boostGrid, nthread=4)
boostTuned$results %>% arrange(ROC)
```

# Model Evaluation
```{R}
logistic_cv = cv.glm(trainDataReduce, logistic_model_final, K=3)
xgb_cv = xgb.cv(trainDataReduce, logistic_model_final, nfold=3)
```
